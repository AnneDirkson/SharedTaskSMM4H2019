{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Two runs for both the UMLfit algorithms "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import json\n",
    "from collections import Counter\n",
    "\n",
    "import pandas as pd \n",
    "import pickle \n",
    "import re\n",
    "import numpy as np \n",
    "from collections import Counter, defaultdict, OrderedDict\n",
    "from nltk import word_tokenize, pos_tag\n",
    "import editdistance\n",
    "import csv \n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import scipy.stats \n",
    "from nltk.corpus import names\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from weighted_levenshtein import lev, osa, dam_lev\n",
    "\n",
    "import langid\n",
    "from nltk.tokenize.treebank import TreebankWordDetokenizer\n",
    "\n",
    "from fastai.text import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               tweet_id                                               text\n",
      "0    649992091716714496  Get Your Free Flu Shot at Hagaman Library!  ht...\n",
      "1    679018395308806145  you know your dedicated to shopping when you g...\n",
      "2    670021606983471104  Japan Eradicate Flu Shots With New Influenza D...\n",
      "3    183869654496854016     14 test positive for H1N1 http://t.co/ALYcvvjM\n",
      "4    230391366474141696  #Health #News A deadly new strain of bird flu ...\n",
      "5    422004338119045120                        I got a flu and I hate it !\n",
      "6    439138046584176640                     Flu causing me to turn skinny.\n",
      "7    540738497510772736  CDC: Flu Shots Less Effective Now Due To Mutat...\n",
      "8    647493241542193154  Humira flu and phemonia :( http://t.co/yXmKrc8EjA\n",
      "9    765397746941620224  I got the flu, but everyone who looking down o...\n",
      "10   261063085656244225  Vaccines: Italy bans Novartis flu pending test...\n",
      "11   516692174738825216  Bloody hell, no idea what they put in them flu...\n",
      "12   520286767204032513  the flu shot gave me the flu, what an incredib...\n",
      "13   409297316349087744  Pandemic flu risk returns, reminiscent of 1950...\n",
      "14   328154782097350656  #malaysian - China reports new bird flu case i...\n",
      "15   608886274397753344  Waking up with the flu has to be the most anno...\n",
      "16   140487723105533953  Man-made super-flu could kill half humanity  R...\n",
      "17   206999481621823488  \"The line was longer than when the swine flu c...\n",
      "18   423843699584290816           The weather is not helping my flu...... \n",
      "19   392534823752835072  Things to Consider Before Getting the Flu Vacc...\n",
      "20   636196608200146944  Universal flu vaccine is no longer science fic...\n",
      "21   428093912637796352  Amid bird flu deaths, Hong Kong culls 20,000 c...\n",
      "22   129437455077216256  @Reek_Wawg Girl u look like a hot cup of soup ...\n",
      "23   623155376221892610  I think im getting the flu or something bc i f...\n",
      "24   532834501236637696  I'm ill..... again..... I can't believe this I...\n",
      "25   279256581227544576  GPBGet your flu shots or nasal-mist vaccine to...\n",
      "26   455876660143718400  I am pretty mad at the staff at Langley Mental...\n",
      "27   508659868153442304  Single Dose Flu Vaccine Found to be Safe and E...\n",
      "28   813541910753120257  I'm getting there. I'm in good sprints at the ...\n",
      "29   506164258950758400  sooooo glad that the flu only lasted for like ...\n",
      "..                  ...                                                ...\n",
      "255  772006089474703360  Not that I'm denying that it's necessary durin...\n",
      "256  753970149598822404  I thought he was spraying for bugs. #Zika http...\n",
      "257  759833988433186818  @RTUKnews shouldn't they already know the risk...\n",
      "258  735467493964275712  #DoYourJob @HouseGOP we don‚Äôt need to roll b...\n",
      "259  776713491839324160  There are 53 *confirmed* cases of the #ZikaVir...\n",
      "260  758379019775598597  My grandparents keep freaking out about Zika a...\n",
      "261  723530661072502784  ‚ÄúWe can‚Äôt spray our way out of this‚Äù @us...\n",
      "262  728649023268130816  @thinkpuertorico and bug spray sounds a better...\n",
      "263  753695588164612096  I hope Congress, will stop playing politics an...\n",
      "264  770263181117849600  Our job is to to protect pregnant woman, Gov. ...\n",
      "265  776786100052488196  @FLGovScott either Zika's gonna kill us here i...\n",
      "266  759038885578113024  Zika has arrived in Florida.   To protect us, ...\n",
      "267  727629849938923521  'We cannot spray our way out of this.' Why cur...\n",
      "268  771490083543388160  @phil500 also they are being poisoned as we sp...\n",
      "269  714074655691259904  #marshablackburn protecting infants lives I do...\n",
      "270  772044528958181376  NFR: Many questions about Zika, spraying, bees...\n",
      "271  767798214980411392  In this informative video HRC tells us Zika is...\n",
      "272  759323462301253632  @SpeakerRyan Where's the money for protecting ...\n",
      "273  694927892233347072                     may Allah protect us from zika\n",
      "274  767556838791389184  Seen people posting angrily about NYC spraying...\n",
      "275  773003927520419840  @MackayIM I mean. I know no immunology, but it...\n",
      "276  756235861696577536  #Zika in the U.S is about to get bigger than p...\n",
      "277  730437357523062784  There is so much about #ZikaVirus we don't kno...\n",
      "278  741081763498229760  Will condoms and repellent be enough to protec...\n",
      "279  771882859829219329  @CNN Anything man puts his hands in/on, he has...\n",
      "280  774261735477903360  It's time to stop playing political games, and...\n",
      "281  791379650022871044  @heyitsmelo I have a question for you: Do you ...\n",
      "282  712706513492512769  Republicans: our first priority is to protect ...\n",
      "283  760656629415174144  Also copped some bug repellent that's 98.11% D...\n",
      "284  760325497687531520  6 more days till I I'm in Jamaica got to start...\n",
      "\n",
      "[285 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "#import the test data\n",
    "\n",
    "path = '/data/dirksonar/Project3_sharedtasks_SMM4H/testdata/testDataST4_participants.txt'\n",
    "test_data = pd.read_csv (path, sep = '\\t', header = None)\n",
    "test_data.columns = ['tweet_id', 'text']\n",
    "print(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = list(test_data['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocessing \n",
    "\n",
    "class Normalizer (): \n",
    "        \n",
    "    def __init__(self): \n",
    "        pass\n",
    "        \n",
    "    #to use this function the files need to be sorted in the same folder as the script under /obj_lex/\n",
    "    def load_obj(self, name):\n",
    "        with open('/home/dirksonar/Scripts/Project1_lexnorm/preprocessing_pipeline/obj_lex/' + name + '.pkl', 'rb') as f:\n",
    "            return pickle.load(f, encoding='latin1')\n",
    "        \n",
    "    def load_files(self): \n",
    "        self.ext_vocab2 = self.load_obj('vocabulary_spelling_unique')\n",
    "        self.abbr_dict = self.load_obj ('abbreviations_dict')\n",
    "        self.celex_freq_dict = self.load_obj ('celex_lwrd_frequencies')\n",
    "        self.celex_list = list(self.celex_freq_dict.keys())\n",
    "        self.celex_set = set (self.celex_list)\n",
    "        self.drug_norm_dict = self.load_obj ('drug_normalize_dict')\n",
    "\n",
    "    def change_tup_to_list(self, tup): \n",
    "        thelist = list(tup)\n",
    "        return thelist\n",
    "    \n",
    "    def change_list_to_tup(self,thelist): \n",
    "        tup = tuple(thelist)\n",
    "        return tup\n",
    "    \n",
    "#---------Remove URls, email addresses and personal pronouns ------------------\n",
    "        \n",
    "    def replace_urls(self,list_of_msgs): \n",
    "        list_of_msgs2 = []\n",
    "        for msg in list_of_msgs: \n",
    "            nw_msg = re.sub(\n",
    "        r'\\b' + r'((\\(<{0,1}https|\\(<{0,1}http|\\[<{0,1}https|\\[<{0,1}http|<{0,1}https|<{0,1}http)(:|;| |: )\\/\\/|www.)[\\w\\.\\/#\\?\\=\\+\\;\\,\\&\\%_\\n-]+(\\.[a-z]{2,4}\\]{0,1}\\){0,1}|\\.html\\]{0,1}\\){0,1}|\\/[\\w\\.\\?\\=#\\+\\;\\,\\&\\%_-]+|[\\w\\/\\.\\?\\=#\\+\\;\\,\\&\\%_-]+|[0-9]+#m[0-9]+)+(\\n|\\b|\\s|\\/|\\]|\\)|>)',\n",
    "        ' ', msg)\n",
    "            list_of_msgs2.append(nw_msg)\n",
    "        return list_of_msgs2    \n",
    "\n",
    "    def replace_email(self,list_of_msgs): \n",
    "        list_of_msgs2 = []\n",
    "        for msg in list_of_msgs: \n",
    "            nw_msg = re.sub (r\"([a-zA-Z0-9_.+-]+@[a-zA-Z0-9-]+\\.[a-zA-Z0-9-.]+[. ])\", ' ', msg) #remove email\n",
    "            nw_msg2 = re.sub (r\"(@[a-zA-Z0-9]+[. ])\", ' ', nw_msg) #remove usernames\n",
    "#             nw_msg3 = re.sub(r\"(@ [a-zA-Z0-9]+[. ])\", ' ', nw_msg2) #remove usernames\n",
    "            list_of_msgs2.append(nw_msg2)\n",
    "        return list_of_msgs2\n",
    "\n",
    "    def remove_empty (self,list_of_msgs): \n",
    "        empty = []\n",
    "        check_msgs3 =[]\n",
    "        for a, i in enumerate (list_of_msgs): \n",
    "            if len(i) == 0: \n",
    "                print('empty')\n",
    "            else: \n",
    "                check_msgs3.append(i)\n",
    "        return check_msgs3\n",
    "    \n",
    "\n",
    "    def create_names_list (self): \n",
    "        male_names = names.words('male.txt')\n",
    "        female_names = names.words('female.txt')\n",
    "        male_set = set (male_names)\n",
    "        female_set = set (female_names)\n",
    "        names_set = male_set.union(female_set) \n",
    "        names_list = [] \n",
    "        for word in names_set: \n",
    "            if (word != 'ned') & (word != 'Ned'): #ned means no evidence and is an important medical term\n",
    "                word1 = str.lower (word)\n",
    "                names_list.append(word1) #add the lowered words\n",
    "                names_list.append(word) #add the capitalized words\n",
    "        \n",
    "        self.names_list = names_list\n",
    "    \n",
    "    def remove_propernoun_names(self,msg):\n",
    "        try: \n",
    "            nw_msg = [self.change_tup_to_list(token) for token in msg]\n",
    "            for a, token in enumerate (nw_msg):\n",
    "                if (token[0] in self.names_list) and ((token[1] == 'NNP') or (token[1]== 'NNPS')): \n",
    "                    new_token = token[0].replace (token[0], \"-NAME-\")\n",
    "                    nw_msg[a] = [new_token, token[1]]\n",
    "#             nw_msg2 = [self.change_list_to_tup(token) for token in nw_msg]\n",
    "            return nw_msg\n",
    "        except TypeError: \n",
    "            pass\n",
    "    \n",
    "    def remove_registered_icon (self, msg): \n",
    "        nw_msg = re.sub ('\\u00AE', '', msg)\n",
    "        nw_msg2 = re.sub ('\\u00E9', 'e', nw_msg)\n",
    "        return nw_msg2\n",
    "    \n",
    "    #this function has been altered because we do not wnat to remove personal pronouns\n",
    "    def anonymize (self, posts): \n",
    "        posts2 = self.replace_urls (posts)\n",
    "        posts3 = self.replace_email (posts2)\n",
    "        posts4 = self.remove_empty(posts3)\n",
    "        posts5 = [self.remove_registered_icon(p) for p in posts4]\n",
    "#         posts5 = [p.encode('latin-1', errors = 'ignore').decode() for p in posts4]\n",
    "        posts6 = [word_tokenize (sent) for sent in posts5]\n",
    "#         posts6 = [pos_tag(sent) for sent in posts5]\n",
    "#         self.create_names_list()\n",
    "#         posts7 = [self.remove_propernoun_names (m) for m in posts6]\n",
    "#         posts8 = []\n",
    "#         for post in posts7: \n",
    "#             tg = [m[0] for m in post]\n",
    "#             posts8.append(tg)\n",
    "        return posts6\n",
    "\n",
    "#---------Convert to lowercase ----------------------------------------------------\n",
    "    \n",
    "    def lowercase (self, post):\n",
    "        post1 = []\n",
    "        for word in post: \n",
    "            word1 = word.lower()\n",
    "            post1.append (word1)\n",
    "        return post1\n",
    "\n",
    "#---------Remove non_English posts -------------------------------------------------    \n",
    "    def language_identify_basic (self, posts):\n",
    "        nw = []\n",
    "        tally = 0\n",
    "        list_removed = []\n",
    "        for post in posts: \n",
    "            out = langid.classify (post)\n",
    "            out2 = list(out)\n",
    "            if out2[0]=='en': \n",
    "                nw.append(post)\n",
    "            else: \n",
    "                tally += 1 \n",
    "                list_removed.append(tuple ([post, out2[0], out2[1]]))\n",
    "        return nw, tally, list_removed\n",
    "    \n",
    "    def language_identify_thres (self, msgs, lang_list, thres):\n",
    "        nw = []\n",
    "        tally = 0\n",
    "        list_removed = []\n",
    "        for post in msgs: \n",
    "            langid.set_languages(lang_list)\n",
    "            out = langid.classify (post)\n",
    "            out2 = list(out)\n",
    "            if out2[0]=='en': \n",
    "                nw.append(post)\n",
    "            elif out2[1] > thres:\n",
    "                nw.append(post)\n",
    "            else: \n",
    "                tally += 1 \n",
    "                list_removed.append(tuple ([post, out2[0], out2[1]]))\n",
    "        return nw, tally, list_removed   \n",
    "\n",
    "    \n",
    "    def remove_non_english(self, posts): \n",
    "        d = TreebankWordDetokenizer()\n",
    "        posts2 = [d.detokenize(m) for m in posts]\n",
    "        \n",
    "        posts_temp, tally, list_removed = self.language_identify_basic(posts2)        \n",
    "        lang = []\n",
    "\n",
    "        for itm in list_removed: \n",
    "            lang.append(itm[1])\n",
    "\n",
    "        c = Counter(lang)\n",
    "\n",
    "        lang_list = ['en']\n",
    "\n",
    "        for itm in c.most_common(10): \n",
    "            z = list(itm)\n",
    "            lang_list.append(z[0])\n",
    "    \n",
    "        print(\"Most common 10 languages in the data are:\" + str(lang_list))\n",
    "        posts3, tally_nw, list_removed_nw = self.language_identify_thres(posts2, lang_list, thres = -100)\n",
    "        return posts3\n",
    "    \n",
    "#---------Lexical normalization pipeline (Sarker, 2017) -------------------------------\n",
    "\n",
    "    def loadItems(self):\n",
    "        '''\n",
    "        This is the primary load function.. calls other loader functions as required..\n",
    "        '''    \n",
    "        global english_to_american\n",
    "        global noslang_dict\n",
    "        global IGNORE_LIST_TRAIN\n",
    "        global IGNORE_LIST\n",
    "\n",
    "        english_to_american = {}\n",
    "        lexnorm_oovs = []\n",
    "        IGNORE_LIST_TRAIN = []\n",
    "        IGNORE_LIST = []\n",
    "\n",
    "        english_to_american = self.loadEnglishToAmericanDict()\n",
    "        noslang_dict = self.loadDictionaryData()\n",
    "        for key, value in noslang_dict.items (): \n",
    "            value2 = value.lower ()\n",
    "            value3 = word_tokenize (value2)\n",
    "            noslang_dict[key] = value3\n",
    "\n",
    "        return None\n",
    "\n",
    "\n",
    "    def loadEnglishToAmericanDict(self):\n",
    "        etoa = {}\n",
    "\n",
    "        english = open('/home/dirksonar/Scripts/Project1_lexnorm/preprocessing_pipeline/obj_lex/englishspellings.txt')\n",
    "        american = open('/home/dirksonar/Scripts/Project1_lexnorm/preprocessing_pipeline/obj_lex/americanspellings.txt')\n",
    "        for line in english:\n",
    "            etoa[line.strip()] = american.readline().strip()\n",
    "        return etoa\n",
    "\n",
    "    def loadDictionaryData(self):\n",
    "        '''\n",
    "        this function loads the various dictionaries which can be used for mapping from oov to iv\n",
    "        '''\n",
    "        n_dict = {}\n",
    "        infile = open('/home/dirksonar/Scripts/Project1_lexnorm/preprocessing_pipeline/obj_lex/noslang_mod.txt')\n",
    "        for line in infile:\n",
    "            items = line.split(' - ')\n",
    "            if len(items[0]) > 0 and len(items) > 1:\n",
    "                n_dict[items[0].strip()] = items[1].strip()\n",
    "        return n_dict\n",
    "\n",
    "\n",
    "    #this has been changed becuase we are dealing with twitter data\n",
    "    def preprocessText(self, tokens, IGNORE_LIST, ignore_username=False, ignore_hashtag=True, ignore_repeated_chars=True, eng_to_am=True, ignore_urls=False):\n",
    "        '''\n",
    "        Note the reason it ignores hashtags, @ etc. is because there is a preprocessing technique that is \n",
    "            designed to remove them \n",
    "        '''\n",
    "        normalized_tokens =[]\n",
    "        #print tokens\n",
    "        text_string = ''\n",
    "        # NOTE: if nesting if/else statements, be careful about execution sequence...\n",
    "        for t in tokens:\n",
    "            t_lower = t.strip().lower()\n",
    "            # if the token is not in the IGNORE_LIST, do various transformations (e.g., ignore usernames and hashtags, english to american conversion\n",
    "            # and others..\n",
    "            if t_lower not in IGNORE_LIST:\n",
    "                # ignore usernames '@'\n",
    "                if re.match('@', t) and ignore_username:\n",
    "                    IGNORE_LIST.append(t_lower)\n",
    "                    text_string += t_lower + ' '\n",
    "                #ignore hashtags\n",
    "                elif re.match('#', t_lower) and ignore_hashtag:\n",
    "                    IGNORE_LIST.append(t_lower)\n",
    "                    text_string += t_lower + ' '\n",
    "                #convert english spelling to american spelling\n",
    "                elif t.strip().lower() in english_to_american.keys() and eng_to_am:    \n",
    "                    text_string += english_to_american[t.strip().lower()] + ' '\n",
    "                #URLS\n",
    "                elif re.search('http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\(\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', t_lower) and ignore_urls:\n",
    "                    IGNORE_LIST.append(t_lower)\n",
    "                    text_string += t_lower + ' '                \n",
    "                elif not ignore_repeated_chars and not re.search(r'[^a-zA-Z]', t_lower):\n",
    "                    # if t_lower only contains alphabetic characters\n",
    "                    t_lower = re.sub(r'([a-z])\\1+', r'\\1\\1', t_lower)\n",
    "                    text_string += t_lower + ' '  \n",
    "                    # print t_lower\n",
    "\n",
    "                # if none of the conditions match, just add the token without any changes..\n",
    "                else:\n",
    "                    text_string += t_lower + ' '\n",
    "            else:  # i.e., if the token is in the ignorelist..\n",
    "                text_string += t_lower + ' '\n",
    "            normalized_tokens = text_string.split()\n",
    "        # print normalized_tokens\n",
    "        return normalized_tokens, IGNORE_LIST\n",
    "\n",
    "\n",
    "    def dictionaryBasedNormalization(self, tokens, I_LIST, M_LIST):\n",
    "        tokens2 =[]\n",
    "        for t in (tokens):\n",
    "            t_lower = t.strip().lower()\n",
    "            if t_lower in noslang_dict.keys() and len(t_lower)>2:\n",
    "                nt = noslang_dict[t_lower]\n",
    "                [tokens2.append(m) for m in nt]\n",
    "\n",
    "                if not t_lower in M_LIST:\n",
    "                    M_LIST.append(t_lower)\n",
    "                if not nt in M_LIST:\n",
    "                    M_LIST.append(nt)\n",
    "            else: \n",
    "                tokens2.append (t)\n",
    "        return tokens2, I_LIST, M_LIST\n",
    "    \n",
    "#----Using the Sarker normalization functions ----------------------------\n",
    "#Step 1 is the English normalization and step 2 is the abbreviation normalization\n",
    "\n",
    "    def normalize_step1(self, tokens, oovoutfile=None):\n",
    "        global IGNORE_LIST\n",
    "        global il\n",
    "        MOD_LIST = []\n",
    "        # Step 1: preprocess the text\n",
    "        normalized_tokens, il = self.preprocessText(tokens, IGNORE_LIST)\n",
    "        normalized_minus_ignorelist = [t for t in normalized_tokens if t not in IGNORE_LIST]\n",
    "        return normalized_minus_ignorelist\n",
    "    \n",
    "    def normalize_step2(self, normalized_tokens, oovoutfile=None): \n",
    "        global IGNORE_LIST\n",
    "        global il\n",
    "        MOD_LIST = []    \n",
    "        ml = MOD_LIST\n",
    "        normalized_tokens, il, ml = self.dictionaryBasedNormalization(normalized_tokens, il, ml)\n",
    "        return normalized_tokens\n",
    "\n",
    "    def sarker_normalize (self,list_of_msgs): \n",
    "        self.loadItems()\n",
    "        msgs_normalized = [self.normalize_step1(m) for m in list_of_msgs]\n",
    "        msgs_normalized2 = [self.normalize_step2(m) for m in msgs_normalized]    \n",
    "        return msgs_normalized2\n",
    "\n",
    "#-------Domain specific abreviation expansion ----------------------------\n",
    "# The list of abbreviations is input as a dictionary with tokenized output  \n",
    "\n",
    "    def domain_specific_abbr (self, tokens, abbr): \n",
    "        post2 = [] \n",
    "        for t in tokens:\n",
    "            if t in abbr.keys(): \n",
    "                nt = abbr[t]\n",
    "                [post2.append(m) for m in nt]\n",
    "            else: \n",
    "                post2.append(t)\n",
    "        return post2\n",
    "\n",
    "    def expand_abbr (self, data, abbr): \n",
    "        data2 = []\n",
    "        for post in data: \n",
    "            post2 = self.domain_specific_abbr (tokens = post, abbr= abbr)\n",
    "            data2.append(post2)\n",
    "        return data2\n",
    "    \n",
    "#-------Spelling correction -------------------------------------------------    \n",
    "    \n",
    "    def load_files2 (self): \n",
    "        #load the edit matrices\n",
    "        #transpositions\n",
    "        self.edits_trans = self.load_obj ('weighted_edits_transpositions')\n",
    "        #deletions \n",
    "        self.edits_del = self.load_obj('weighted_edits_deletions')\n",
    "        #insertions \n",
    "        self.edits_ins = self.load_obj('weighted_edits_insertions')\n",
    "        #substitutions\n",
    "        self.edits_sub = self.load_obj('weighted_edits_substitutions')\n",
    "                \n",
    "        #load the generic dictionary - CHANGE PATH!  \n",
    "        self.celex_freq_dict = self.load_obj ('celex_lwrd_frequencies')\n",
    "    \n",
    "    \n",
    "    def initialize_weighted_matrices(self): \n",
    "    #initialize the cost matrixes for deletions and insertions\n",
    "        insert_costs = np.ones(128, dtype=np.float64)  # make an array of all 1's of size 128, the number of ASCII characters\n",
    "        delete_costs = np.ones (128, dtype=np.float64)\n",
    "\n",
    "        for index,row in self.edits_ins.iterrows(): \n",
    "            insert_costs[ord(index)] = row['transformed_frequency']\n",
    "\n",
    "        for index,row in self.edits_del.iterrows(): \n",
    "            delete_costs[ord(index)] = row['transformed_frequency']\n",
    "\n",
    "        #substitution\n",
    "\n",
    "        substitute_costs = np.ones((128, 128), dtype=np.float64)\n",
    "        lst = []\n",
    "        for index,row in self.edits_sub.iterrows(): \n",
    "            z = tuple([row['edit_from'], row['edit_to'], row['transformed_frequency']])\n",
    "            lst.append (z)\n",
    "        for itm in lst: \n",
    "            itm2 = list(itm)\n",
    "            try: \n",
    "                substitute_costs[ord(itm2[0]), ord(itm2[1])] = itm2[2]\n",
    "            except IndexError: \n",
    "                pass\n",
    "\n",
    "        #transposition\n",
    "\n",
    "        transpose_costs = np.ones((128, 128), dtype=np.float64)\n",
    "\n",
    "        lst = []\n",
    "\n",
    "        for index,row in self.edits_trans.iterrows(): \n",
    "            z = tuple([row['first_letter'], row['second_letter'], row['transformed_frequency']])\n",
    "            lst.append (z)\n",
    "\n",
    "        for itm in lst: \n",
    "            itm2 = list(itm)\n",
    "            try: \n",
    "                transpose_costs[ord(itm2[0]), ord(itm2[1])] = itm2[2]\n",
    "            except IndexError: \n",
    "                print(itm2)\n",
    "\n",
    "        return insert_costs, delete_costs, substitute_costs, transpose_costs\n",
    "\n",
    "    \n",
    "    def weighted_ed_rel (self, cand, token, del_costs, ins_costs, sub_costs, trans_costs): \n",
    "        try: \n",
    "            w_editdist = dam_lev(token, cand, delete_costs = del_costs, insert_costs = ins_costs, substitute_costs = sub_costs, transpose_costs = trans_costs)\n",
    "            rel_w_editdist = w_editdist/len(token)\n",
    "            return rel_w_editdist\n",
    "        except UnicodeEncodeError: \n",
    "#             print(token)\n",
    "            IGNORE_LIST.append(token)\n",
    "            rel_w_editdist = 100\n",
    "            return rel_w_editdist\n",
    "    \n",
    "\n",
    "    def run_low (self, word, voc, func, del_costs, ins_costs, sub_costs, trans_costs): \n",
    "        replacement = [' ',100]\n",
    "        for token in voc: \n",
    "            sim = func(word, token, del_costs, ins_costs, sub_costs, trans_costs)\n",
    "            if sim < replacement[1]:\n",
    "                replacement[1] = sim\n",
    "                replacement[0] = token\n",
    "\n",
    "        return replacement   \n",
    "    \n",
    "    \n",
    "    def spelling_correction (self, post, token_freq_dict, token_freq_ordered, min_rel_freq = 2, max_rel_edit_dist = 0.08): \n",
    "        post2 = []\n",
    "        cnt = 0 \n",
    "\n",
    "        for a, token in enumerate (post): \n",
    "            if self.TRUE_WORD.fullmatch(token):\n",
    "                if token in self.spelling_corrections:\n",
    "                    correct = self.spelling_corrections[token] \n",
    "                    post2.append(correct)\n",
    "                    cnt +=1\n",
    "                    self.replaced.append(token)\n",
    "                    self.replaced_with.append(correct)\n",
    "\n",
    "                elif token in self.celex_freq_dict:\n",
    "                    post2.append(token)\n",
    "\n",
    "                else:\n",
    "\n",
    "                    # make the subset of possible candidates\n",
    "                    freq_word = token_freq_dict[token]\n",
    "                    limit = freq_word * min_rel_freq\n",
    "                    subset = [t[0] for t in token_freq_ordered if t[1]>= limit]\n",
    "\n",
    "                    #compare these candidates with the word        \n",
    "                    candidate = self.run_low (token, subset, self.weighted_ed_rel, self.delete_costs_nw, self.insert_costs_nw, \n",
    "                                         self.substitute_costs_nw, self.transpose_costs_nw)\n",
    "\n",
    "                #if low enough RE - candidate is deemed good\n",
    "                    if candidate[1] <= max_rel_edit_dist:\n",
    "                        post2.append(candidate[0]) \n",
    "                        cnt +=1\n",
    "                        self.replaced.append(token)\n",
    "                        self.replaced_with.append(candidate[0])\n",
    "                        self.spelling_corrections [token] = candidate[0]\n",
    "                    else: \n",
    "                        post2.append(token)\n",
    "            else: post2.append(token)\n",
    "        self.total_cnt.append (cnt)\n",
    "        return post2\n",
    "      \n",
    "    def initialize_files_for_spelling(self): \n",
    "        total_cnt = []\n",
    "        replaced = []\n",
    "        replaced_with = []\n",
    "        spelling_corrections= {}\n",
    "        return total_cnt, replaced, replaced_with, spelling_corrections\n",
    "    \n",
    "    def change_tup_to_list (self, tup): \n",
    "        thelist = list(tup)\n",
    "        return thelist\n",
    "\n",
    "    def create_token_freq (self, data): \n",
    "        flat_data = [item for sublist in data for item in sublist]\n",
    "        self.token_freq = Counter(flat_data)\n",
    "        \n",
    "        token_freq_ordered = self.token_freq.most_common ()\n",
    "        self.token_freq_ordered2 = [self.change_tup_to_list(m) for m in token_freq_ordered]\n",
    "    \n",
    "    def correct_spelling_mistakes(self, data): \n",
    "#         data= self.load_obj ('/data/dirksonar/Project1_lexnorm/spelling_correction/output/', 'gistdata_lemmatised')\n",
    "        self.load_files2()\n",
    "        self.insert_costs_nw, self.delete_costs_nw, self.substitute_costs_nw, self.transpose_costs_nw = self.initialize_weighted_matrices()\n",
    "        self.total_cnt, self.replaced, self.replaced_with, self.spelling_corrections= self.initialize_files_for_spelling()\n",
    "        self.TRUE_WORD = re.compile('[-a-z]+')  # Only letters and dashes  \n",
    "#         data2 = [word_tokenize(m) for m in data]\n",
    "        self.create_token_freq(data)\n",
    "        out = [self.spelling_correction (m, self.token_freq, self.token_freq_ordered2) for m in data]\n",
    "        return out, self.total_cnt, self.replaced, self.replaced_with, self.spelling_corrections\n",
    "    \n",
    "#--------Overall normalization function--------------------------------------\n",
    "    \n",
    "    def normalize (self, posts): \n",
    "        self.load_files ()\n",
    "        posts1 = self.anonymize(posts)\n",
    "        posts2 = [self.lowercase (m) for m in posts1]\n",
    "#         posts3 = self.remove_non_english (posts2)\n",
    "#         posts3 = [word_tokenize(m) for m in posts2]\n",
    "        posts4 = [self.sarker_normalize(posts2)]\n",
    "        posts5 = [self.expand_abbr(posts4[0], self.abbr_dict)]\n",
    "#         posts6, total_cnt, replaced, replaced_with, spelling_corrections = self.correct_spelling_mistakes(posts5[0])\n",
    "        return posts5[0]\n",
    "\n",
    "    def normalize_extra(self, posts): \n",
    "        self.load_files()\n",
    "        self.loadItems()\n",
    "        posts2, total_cnt, replaced, replaced_with, spelling_corrections_nw = self.correct_spelling_mistakes(posts)\n",
    "        posts_ignored = []\n",
    "        for post in posts2: \n",
    "            p2 = [t for t in post if t not in IGNORE_LIST]\n",
    "            posts_ignored.append(p2)\n",
    "        return posts_ignored, total_cnt, replaced, replaced_with, spelling_corrections_nw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt_norm1 = Normalizer().normalize(txt)\n",
    "txt_norm2, total_cnt, replaced, replaced_with, spelling_corrections_nw = Normalizer().normalize_extra(txt_norm1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'newsa': 'news', 'kong': 'kang', 'ave': 'have', 'fely': 'fly', 'gt': 'get', 'carribbean': 'caribbean', 'th': 'the', 'usaid': 'said', 'unprotected': 'protected'}\n"
     ]
    }
   ],
   "source": [
    "# print(spelling_corrections_nw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [print(m) for m in txt_norm2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punc (post): \n",
    "    temp = []\n",
    "    for word in post: \n",
    "        if re.fullmatch (r'[^\\w\\s]', word) == None: \n",
    "            temp.append (word)\n",
    "        else: \n",
    "            pass\n",
    "    return temp \n",
    "\n",
    "# (u\"\\2026\", '')\n",
    "\n",
    "def post_filter_char (msg):\n",
    "    final1 = msg.replace('Â', '')\n",
    "    final2= final1.replace('â€™', '')\n",
    "    final3 = final2.replace('â€œ', '')\n",
    "    final4 = final3.replace('â€“', '')\n",
    "    final5 = final4.replace('â€¦', '')\n",
    "    final6 = final5.replace('â€', '')\n",
    "    final7 = final6.replace('...', '')\n",
    "    final8 = final7.replace ('`', '')\n",
    "    final9 = final8.replace ('ðÿ˜', '')\n",
    "    final10 = final9.replace ('¡', '')\n",
    "    final11 = final10.replace ('©', '')\n",
    "    final12 = re.sub(r'(@ ?[a-zA-Z0-9-_]+[\\.: ]?)', '', final11)\n",
    "    return final12\n",
    "\n",
    "txt_norm3 = [remove_punc(m) for m in txt_norm2]\n",
    "d = TreebankWordDetokenizer ()\n",
    "txt_norm4 = [d.detokenize(m) for m in txt_norm3]\n",
    "txt_norm5 = [post_filter_char(m) for m in txt_norm4]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Fbeta_binary(Callback):\n",
    "    \"Computes the fbeta between preds and targets for single-label classification\"\n",
    "    beta2: int = 2\n",
    "    eps: float = 1e-9\n",
    "    clas:int=1\n",
    "    \n",
    "    def on_epoch_begin(self, **kwargs):\n",
    "        self.TP = 0\n",
    "        self.total_y_pred = 0   \n",
    "        self.total_y_true = 0\n",
    "    \n",
    "    def on_batch_end(self, last_output, last_target, **kwargs):\n",
    "        y_pred = last_output.argmax(dim=1)\n",
    "        y_true = last_target.float()\n",
    "        \n",
    "        self.TP += ((y_pred==self.clas) * (y_true==self.clas)).float().sum()\n",
    "        self.total_y_pred += (y_pred==self.clas).float().sum()\n",
    "        self.total_y_true += (y_true==self.clas).float().sum()\n",
    "    \n",
    "    def on_epoch_end(self, last_metrics, **kwargs):\n",
    "        beta2=self.beta2**2\n",
    "        prec = self.TP/(self.total_y_pred+self.eps)\n",
    "        rec = self.TP/(self.total_y_true+self.eps)       \n",
    "        res = (prec*rec)/(prec*beta2+rec+self.eps)*(1+beta2)\n",
    "        self.metric = res \n",
    "        return add_metrics(last_metrics, self.metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(txt_norm5)\n",
    "#run ULMfit algorithm1 \n",
    "path = '/data/dirksonar/Project3_sharedtasks_SMM4H/Task4/fastai/'\n",
    "learn = load_learner (path, 'classifier_phm_1.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label (output): \n",
    "    out = []\n",
    "    for i in output: \n",
    "        lst = list(i)\n",
    "        z = str(lst[0])\n",
    "        label = z[-1]\n",
    "        out.append(float(label))\n",
    "    return out\n",
    "\n",
    "predicted = [learn.predict (i) for i in txt_norm5]\n",
    "pred_labels = extract_label(predicted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(pred_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run UMlfit algorithm2\n",
    "path = '/data/dirksonar/Project3_sharedtasks_SMM4H/umlfit_languagemodel/'\n",
    "learn = load_learner (path, 'classifier_phm_2.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_label (output): \n",
    "    out = []\n",
    "    for i in output: \n",
    "        lst = list(i)\n",
    "        z = str(lst[0])\n",
    "        label = z[-1]\n",
    "        out.append(float(label))\n",
    "    return out\n",
    "\n",
    "predicted2 = [learn.predict (i) for i in txt_norm5]\n",
    "pred_labels2 = extract_label(predicted2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9298245614035088\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print(accuracy_score(pred_labels, pred_labels2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_id</th>\n",
       "      <th>text</th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>649992091716714496</td>\n",
       "      <td>Get Your Free Flu Shot at Hagaman Library!  ht...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679018395308806145</td>\n",
       "      <td>you know your dedicated to shopping when you g...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>670021606983471104</td>\n",
       "      <td>Japan Eradicate Flu Shots With New Influenza D...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183869654496854016</td>\n",
       "      <td>14 test positive for H1N1 http://t.co/ALYcvvjM</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230391366474141696</td>\n",
       "      <td>#Health #News A deadly new strain of bird flu ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             tweet_id                                               text    0\n",
       "0  649992091716714496  Get Your Free Flu Shot at Hagaman Library!  ht...  0.0\n",
       "1  679018395308806145  you know your dedicated to shopping when you g...  0.0\n",
       "2  670021606983471104  Japan Eradicate Flu Shots With New Influenza D...  0.0\n",
       "3  183869654496854016     14 test positive for H1N1 http://t.co/ALYcvvjM  0.0\n",
       "4  230391366474141696  #Health #News A deadly new strain of bird flu ...  0.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save output in correct manner \n",
    "\n",
    "output_test_task4_run1 = pd.concat([test_data, pd.Series(pred_labels)], axis = 1)\n",
    "output_test_task4_run2 = pd.concat([test_data, pd.Series(pred_labels2)], axis = 1)\n",
    "\n",
    "output_test_task4_run1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_obj(obj, name):\n",
    "    with open(name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "        \n",
    "path = '/data/dirksonar/Project3_sharedtasks_SMM4H/testdata/output_test_task4_run1'\n",
    "path2 = '/data/dirksonar/Project3_sharedtasks_SMM4H/testdata/output_test_task4_run2'\n",
    "\n",
    "save_obj(output_test_task4_run1, path)\n",
    "save_obj(output_test_task4_run2, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tweet ID</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>649992091716714496</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>679018395308806145</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>670021606983471104</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>183869654496854016</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>230391366474141696</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>422004338119045120</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>439138046584176640</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>540738497510772736</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>647493241542193154</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>765397746941620224</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>261063085656244225</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>516692174738825216</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>520286767204032513</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>409297316349087744</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>328154782097350656</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>608886274397753344</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>140487723105533953</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>206999481621823488</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>423843699584290816</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>392534823752835072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>636196608200146944</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>428093912637796352</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>129437455077216256</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>623155376221892610</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>532834501236637696</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>279256581227544576</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>455876660143718400</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>508659868153442304</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>813541910753120257</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>506164258950758400</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>255</th>\n",
       "      <td>772006089474703360</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>256</th>\n",
       "      <td>753970149598822404</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>257</th>\n",
       "      <td>759833988433186818</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>735467493964275712</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>776713491839324160</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>758379019775598597</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>723530661072502784</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>728649023268130816</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>753695588164612096</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>264</th>\n",
       "      <td>770263181117849600</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>265</th>\n",
       "      <td>776786100052488196</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>266</th>\n",
       "      <td>759038885578113024</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>727629849938923521</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>771490083543388160</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>714074655691259904</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>772044528958181376</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>767798214980411392</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>759323462301253632</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>694927892233347072</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>767556838791389184</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>773003927520419840</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>756235861696577536</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>730437357523062784</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>741081763498229760</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>771882859829219329</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>774261735477903360</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>791379650022871044</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>712706513492512769</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>760656629415174144</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>760325497687531520</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>285 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Tweet ID  Label\n",
       "0    649992091716714496    0.0\n",
       "1    679018395308806145    0.0\n",
       "2    670021606983471104    0.0\n",
       "3    183869654496854016    0.0\n",
       "4    230391366474141696    0.0\n",
       "5    422004338119045120    1.0\n",
       "6    439138046584176640    1.0\n",
       "7    540738497510772736    0.0\n",
       "8    647493241542193154    0.0\n",
       "9    765397746941620224    1.0\n",
       "10   261063085656244225    0.0\n",
       "11   516692174738825216    1.0\n",
       "12   520286767204032513    1.0\n",
       "13   409297316349087744    0.0\n",
       "14   328154782097350656    0.0\n",
       "15   608886274397753344    1.0\n",
       "16   140487723105533953    0.0\n",
       "17   206999481621823488    0.0\n",
       "18   423843699584290816    1.0\n",
       "19   392534823752835072    0.0\n",
       "20   636196608200146944    0.0\n",
       "21   428093912637796352    0.0\n",
       "22   129437455077216256    0.0\n",
       "23   623155376221892610    1.0\n",
       "24   532834501236637696    1.0\n",
       "25   279256581227544576    0.0\n",
       "26   455876660143718400    0.0\n",
       "27   508659868153442304    0.0\n",
       "28   813541910753120257    1.0\n",
       "29   506164258950758400    1.0\n",
       "..                  ...    ...\n",
       "255  772006089474703360    0.0\n",
       "256  753970149598822404    0.0\n",
       "257  759833988433186818    0.0\n",
       "258  735467493964275712    0.0\n",
       "259  776713491839324160    0.0\n",
       "260  758379019775598597    1.0\n",
       "261  723530661072502784    0.0\n",
       "262  728649023268130816    0.0\n",
       "263  753695588164612096    0.0\n",
       "264  770263181117849600    0.0\n",
       "265  776786100052488196    0.0\n",
       "266  759038885578113024    0.0\n",
       "267  727629849938923521    0.0\n",
       "268  771490083543388160    0.0\n",
       "269  714074655691259904    0.0\n",
       "270  772044528958181376    0.0\n",
       "271  767798214980411392    0.0\n",
       "272  759323462301253632    0.0\n",
       "273  694927892233347072    0.0\n",
       "274  767556838791389184    0.0\n",
       "275  773003927520419840    0.0\n",
       "276  756235861696577536    0.0\n",
       "277  730437357523062784    0.0\n",
       "278  741081763498229760    0.0\n",
       "279  771882859829219329    0.0\n",
       "280  774261735477903360    0.0\n",
       "281  791379650022871044    0.0\n",
       "282  712706513492512769    0.0\n",
       "283  760656629415174144    0.0\n",
       "284  760325497687531520    1.0\n",
       "\n",
       "[285 rows x 2 columns]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_test_task4_run1_notext = output_test_task4_run1.drop('text', axis =1)\n",
    "output_test_task4_run1_notext.columns =['Tweet ID', 'Label']\n",
    "output_test_task4_run1_notext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test_task4_run2_notext = output_test_task4_run2.drop('text', axis =1)\n",
    "output_test_task4_run2_notext.columns =['Tweet ID', 'Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/dirksonar/Project3_sharedtasks_SMM4H/testdata/output_test_task4_run1_notext'\n",
    "path2 = '/data/dirksonar/Project3_sharedtasks_SMM4H/testdata/output_test_task4_run2_notext'\n",
    "\n",
    "save_obj(output_test_task4_run1_notext, path)\n",
    "save_obj(output_test_task4_run2_notext, path2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '/data/dirksonar/Project3_sharedtasks_SMM4H/testdata/output_test_task4_run1_notext.txt'\n",
    "path2 = '/data/dirksonar/Project3_sharedtasks_SMM4H/testdata/output_test_task4_run2_notext.txt'\n",
    "\n",
    "output_test_task4_run1_notext.to_csv (path, index = False, sep= '\\t', header = False)\n",
    "output_test_task4_run2_notext.to_csv (path2, index = False, sep= '\\t', header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
